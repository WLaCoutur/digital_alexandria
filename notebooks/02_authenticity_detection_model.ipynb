{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Digital Alexandria: Artwork Authenticity Detection Model\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Objective**: Build and train a machine learning model to detect artwork authenticity using computer vision and deep learning techniques.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Author**: William Couturier  \\n\",\n",
    "    \"**Date**: 2024\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import libraries\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, cross_val_score\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler, LabelEncoder\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.svm import SVC\\n\",\n",
    "    \"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\\n\",\n",
    "    \"import tensorflow as tf\\n\",\n",
    "    \"from tensorflow import keras\\n\",\n",
    "    \"from tensorflow.keras import layers\\n\",\n",
    "    \"import cv2\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üé® Digital Alexandria - Authenticity Detection Model\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 55)\\n\",\n",
    "    \"print(f\\\"TensorFlow version: {tf.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"GPU Available: {tf.config.list_physical_devices('GPU')}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìä Data Preparation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate synthetic artwork features dataset\\n\",\n",
    "    \"def generate_artwork_features(n_samples=5000):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Generate synthetic artwork features for authenticity detection\\n\",\n",
    "    \"    Features include: color analysis, texture patterns, brush strokes, age indicators\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    np.random.seed(42)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Authentic artworks (70% of dataset)\\n\",\n",
    "    \"    n_authentic = int(n_samples * 0.7)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Color features (RGB histogram statistics)\\n\",\n",
    "    \"    color_variance_auth = np.random.normal(0.15, 0.05, n_authentic)  # Authentic art has natural color variance\\n\",\n",
    "    \"    color_variance_fake = np.random.normal(0.25, 0.08, n_samples - n_authentic)  # Fakes often have irregular colors\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Texture complexity (entropy-based measure)\\n\",\n",
    "    \"    texture_complexity_auth = np.random.normal(7.2, 1.1, n_authentic)  # Natural brush patterns\\n\",\n",
    "    \"    texture_complexity_fake = np.random.normal(5.8, 1.5, n_samples - n_authentic)  # Simpler or too complex\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Brush stroke consistency\\n\",\n",
    "    \"    brush_consistency_auth = np.random.normal(0.8, 0.1, n_authentic)  # Consistent artist technique\\n\",\n",
    "    \"    brush_consistency_fake = np.random.normal(0.6, 0.15, n_samples - n_authentic)  # Inconsistent technique\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Age indicators (cracking patterns, patina)\\n\",\n",
    "    \"    age_indicators_auth = np.random.normal(0.7, 0.2, n_authentic)  # Natural aging\\n\",\n",
    "    \"    age_indicators_fake = np.random.normal(0.3, 0.25, n_samples - n_authentic)  # Artificial aging or too new\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Canvas/material analysis\\n\",\n",
    "    \"    canvas_authenticity_auth = np.random.normal(0.85, 0.1, n_authentic)  # Period-appropriate materials\\n\",\n",
    "    \"    canvas_authenticity_fake = np.random.normal(0.45, 0.2, n_samples - n_authentic)  # Modern materials\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Pigment analysis (spectroscopic features)\\n\",\n",
    "    \"    pigment_match_auth = np.random.normal(0.9, 0.08, n_authentic)  # Period-correct pigments\\n\",\n",
    "    \"    pigment_match_fake = np.random.normal(0.5, 0.2, n_samples - n_authentic)  # Modern pigments\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Style consistency (deep learning feature)\\n\",\n",
    "    \"    style_match_auth = np.random.normal(0.88, 0.1, n_authentic)  # Matches artist's style\\n\",\n",
    "    \"    style_match_fake = np.random.normal(0.4, 0.18, n_samples - n_authentic)  # Style mismatch\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Provenance score (historical documentation)\\n\",\n",
    "    \"    provenance_auth = np.random.normal(0.75, 0.15, n_authentic)  # Good documentation\\n\",\n",
    "    \"    provenance_fake = np.random.normal(0.2, 0.15, n_samples - n_authentic)  # Poor/missing documentation\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Combine features\\n\",\n",
    "    \"    features_authentic = np.column_stack([\\n\",\n",
    "    \"        color_variance_auth, texture_complexity_auth, brush_consistency_auth,\\n\",\n",
    "    \"        age_indicators_auth, canvas_authenticity_auth, pigment_match_auth,\\n\",\n",
    "    \"        style_match_auth, provenance_auth\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    features_fake = np.column_stack([\\n\",\n",
    "    \"        color_variance_fake, texture_complexity_fake, brush_consistency_fake,\\n\",\n",
    "    \"        age_indicators_fake, canvas_authenticity_fake, pigment_match_fake,\\n\",\n",
    "    \"        style_match_fake, provenance_fake\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Labels\\n\",\n",
    "    \"    labels_authentic = np.ones(n_authentic)\\n\",\n",
    "    \"    labels_fake = np.zeros(n_samples - n_authentic)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Combine and shuffle\\n\",\n",
    "    \"    X = np.vstack([features_authentic, features_fake])\\n\",\n",
    "    \"    y = np.hstack([labels_authentic, labels_fake])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Shuffle\\n\",\n",
    "    \"    indices = np.random.permutation(len(X))\\n\",\n",
    "    \"    X, y = X[indices], y[indices]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create DataFrame\\n\",\n",
    "    \"    feature_names = [\\n\",\n",
    "    \"        'color_variance', 'texture_complexity', 'brush_consistency',\\n\",\n",
    "    \"        'age_indicators', 'canvas_authenticity', 'pigment_match',\\n\",\n",
    "    \"        'style_match', 'provenance_score'\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    df = pd.DataFrame(X, columns=feature_names)\\n\",\n",
    "    \"    df['is_authentic'] = y\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return df\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate dataset\\n\",\n",
    "    \"artwork_data = generate_artwork_features(5000)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"üìä Dataset generated: {len(artwork_data)} samples\\\")\\n\",\n",
    "    \"print(f\\\"üé® Authentic artworks: {artwork_data['is_authentic'].sum():.0f} ({artwork_data['is_authentic'].mean()*100:.1f}%)\\\")\\n\",\n",
    "    \"print(f\\\"üîç Suspected fakes: {(1-artwork_data['is_authentic']).sum():.0f} ({(1-artwork_data['is_authentic']).mean()*100:.1f}%)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display sample\\n\",\n",
    "    \"display(artwork_data.head(10))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìà Exploratory Data Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature distributions by authenticity\\n\",\n",
    "    \"fig, axes = plt.subplots(2, 4, figsize=(20, 10))\\n\",\n",
    "    \"axes = axes.ravel()\\n\",\n",
    "    \"\\n\",\n",
    "    \"feature_cols = [col for col in artwork_data.columns if col != 'is_authentic']\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, feature in enumerate(feature_cols):\\n\",\n",
    "    \"    authentic_data = artwork_data[artwork_data['is_authentic'] == 1][feature]\\n\",\n",
    "    \"    fake_data = artwork_data[artwork_data['is_authentic'] == 0][feature]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    axes[i].hist(authentic_data, alpha=0.7, label='Authentic', bins=30, color='green')\\n\",\n",
    "    \"    axes[i].hist(fake_data, alpha=0.7, label='Fake', bins=30, color='red')\\n\",\n",
    "    \"    axes[i].set_title(f'{feature.replace(\\\"_\\\", \\\" \\\").title()}')\\n\",\n",
    "    \"    axes[i].legend()\\n\",\n",
    "    \"    axes[i].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.suptitle('üé® Feature Distributions: Authentic vs Fake Artworks', y=1.02, fontsize=16)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Correlation matrix\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"correlation_matrix = artwork_data.corr()\\n\",\n",
    "    \"sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, \\n\",\n",
    "    \"            square=True, linewidths=0.5)\\n\",\n",
    "    \"plt.title('üîç Feature Correlation Matrix')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Statistical summary\\n\",\n",
    "    \"print(\\\"üìä Statistical Summary by Authenticity:\\\")\\n\",\n",
    "    \"print(\\\"\\\\nüé® AUTHENTIC ARTWORKS:\\\")\\n\",\n",
    "    \"print(artwork_data[artwork_data['is_authentic'] == 1].describe())\\n\",\n",
    "    \"print(\\\"\\\\nüîç SUSPECTED FAKES:\\\")\\n\",\n",
    "    \"print(artwork_data[artwork_data['is_authentic'] == 0].describe())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ü§ñ Model Development\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prepare data for modeling\\n\",\n",
    "    \"X = artwork_data.drop('is_authentic', axis=1)\\n\",\n",
    "    \"y = artwork_data['is_authentic']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train-test split\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    X, y, test_size=0.2, random_state=42, stratify=y\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Feature scaling\\n\",\n",
    "    \"scaler = StandardScaler()\\n\",\n",
    "    \"X_train_scaled = scaler.fit_transform(X_train)\\n\",\n",
    "    \"X_test_scaled = scaler.transform(X_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"üéØ Training set: {len(X_train)} samples\\\")\\n\",\n",
    "    \"print(f\\\"üß™ Test set: {len(X_test)} samples\\\")\\n\",\n",
    "    \"print(f\\\"üìä Feature dimensions: {X_train.shape[1]}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize models\\n\",\n",
    "    \"models = {\\n\",\n",
    "    \"    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\\n\",\n",
    "    \"    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\\n\",\n",
    "    \"    'Logistic Regression': LogisticRegression(random_state=42),\\n\",\n",
    "    \"    'SVM': SVC(kernel='rbf', probability=True, random_state=42)\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train and evaluate models\\n\",\n",
    "    \"model_results = {}\\n\",\n",
    "    \"\\n\",\n",
    "    \"for name, model in models.items():\\n\",\n",
    "    \"    print(f\\\"\\\\nü§ñ Training {name}...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Use scaled data for SVM and Logistic Regression\\n\",\n",
    "    \"    if name in ['SVM', 'Logistic Regression']:\\n\",\n",
    "    \"        model.fit(X_train_scaled, y_train)\\n\",\n",
    "    \"        y_pred = model.predict(X_test_scaled)\\n\",\n",
    "    \"        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        model.fit(X_train, y_train)\\n\",\n",
    "    \"        y_pred = model.predict(X_test)\\n\",\n",
    "    \"        y_pred_proba = model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate metrics\\n\",\n",
    "    \"    accuracy = accuracy_score(y_test, y_pred)\\n\",\n",
    "    \"    precision = precision_score(y_test, y_pred)\\n\",\n",
    "    \"    recall = recall_score(y_test, y_pred)\\n\",\n",
    "    \"    f1 = f1_score(y_test, y_pred)\\n\",\n",
    "    \"    auc = roc_auc_score(y_test, y_pred_proba)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    model_results[name] = {\\n\",\n",
    "    \"        'model': model,\\n\",\n",
    "    \"        'accuracy': accuracy,\\n\",\n",
    "    \"        'precision': precision,\\n\",\n",
    "    \"        'recall': recall,\\n\",\n",
    "    \"        'f1': f1,\\n\",\n",
    "    \"        'auc': auc,\\n\",\n",
    "    \"        'predictions': y_pred,\\n\",\n",
    "    \"        'probabilities': y_pred_proba\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"‚úÖ Accuracy: {accuracy:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"üìä Precision: {precision:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"üéØ Recall: {recall:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"‚öñÔ∏è F1-Score: {f1:.3f}\\\")\\n\",\n",
    "    \"    print(f\\\"üìà AUC: {auc:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìä Model Comparison and Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create comparison DataFrame\\n\",\n",
    "    \"comparison_data = []\\n\",\n",
    "    \"for name, results in model_results.items():\\n\",\n",
    "    \"    comparison_data.append({\\n\",\n",
    "    \"        'Model': name,\\n\",\n",
    "    \"        'Accuracy': results['accuracy'],\\n\",\n",
    "    \"        'Precision': results['precision'],\\n\",\n",
    "    \"        'Recall': results['recall'],\\n\",\n",
    "    \"        'F1-Score': results['f1'],\\n\",\n",
    "    \"        'AUC': results['auc']\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df = pd.DataFrame(comparison_data)\\n\",\n",
    "    \"comparison_df = comparison_df.sort_values('AUC', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üèÜ MODEL PERFORMANCE COMPARISON:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"display(comparison_df.round(3))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize model comparison\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Bar plot of metrics\\n\",\n",
    "    \"metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']\\n\",\n",
    "    \"x = np.arange(len(comparison_df))\\n\",\n",
    "    \"width = 0.15\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, metric in enumerate(metrics_to_plot):\\n\",\n",
    "    \"    axes[0].bar(x + i*width, comparison_df[metric], width, label=metric, alpha=0.8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"axes[0].set_xlabel('Models')\\n\",\n",
    "    \"axes[0].set_ylabel('Score')\\n\",\n",
    "    \"axes[0].set_title('üèÜ Model Performance Comparison')\\n\",\n",
    "    \"axes[0].set_xticks(x + width * 2)\\n\",\n",
    "    \"axes[0].set_xticklabels(comparison_df['Model'], rotation=45)\\n\",\n",
    "    \"axes[0].legend()\\n\",\n",
    "    \"axes[0].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Best model confusion matrix\\n\",\n",
    "    \"best_model_name = comparison_df.iloc[0]['Model']\\n\",\n",
    "    \"best_predictions = model_results[best_model_name]['predictions']\\n\",\n",
    "    \"\\n\",\n",
    "    \"cm = confusion_matrix(y_test, best_predictions)\\n\",\n",
    "    \"sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\\n\",\n",
    "    \"axes[1].set_title(f'üéØ Confusion Matrix - {best_model_name}')\\n\",\n",
    "    \"axes[1].set_xlabel('Predicted')\\n\",\n",
    "    \"axes[1].set_ylabel('Actual')\\n\",\n",
    "    \"axes[1].set_xticklabels(['Fake', 'Authentic'])\\n\",\n",
    "    \"axes[1].set_yticklabels(['Fake', 'Authentic'])\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nü•á BEST MODEL: {best_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"üìä AUC Score: {comparison_df.iloc[0]['AUC']:.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üß† Deep Learning Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Build neural network for authenticity detection\\n\",\n",
    "    \"def create_authenticity_model(input_dim):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Create a neural network for artwork authenticity detection\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    model = keras.Sequential([\\n\",\n",
    "    \"        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\\n\",\n",
    "    \"        layers.BatchNormalization(),\\n\",\n",
    "    \"        layers.Dropout(0.3),\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        layers.Dense(64, activation='relu'),\\n\",\n",
    "    \"        layers.BatchNormalization(),\\n\",\n",
    "    \"        layers.Dropout(0.3),\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        layers.Dense(32, activation='relu'),\\n\",\n",
    "    \"        layers.Dropout(0.2),\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        layers.Dense(16, activation='relu'),\\n\",\n",
    "    \"        layers.Dense(1, activation='sigmoid')  # Binary classification\\n\",\n",
    "    \"    ])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    model.compile(\\n\",\n",
    "    \"        optimizer=keras.optimizers.Adam(learning_rate=0.001),\\n\",\n",
    "    \"        loss='binary_crossentropy',\\n\",\n",
    "    \"        metrics=['accuracy', 'precision', 'recall']\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return model\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create and train neural network\\n\",\n",
    "    \"print(\\\"üß† Building Deep Learning Model...\\\")\\n\",\n",
    "    \"nn_model = create_authenticity_model(X_train_scaled.shape[1])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display model architecture\\n\",\n",
    "    \"print(\\\"\\\\nüèóÔ∏è Model Architecture:\\\")\\n\",\n",
    "    \"nn_model.summary()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Train model\\n\",\n",
    "    \"print(\\\"\\\\nüéØ Training Neural Network...\\\")\\n\",\n",
    "    \"history = nn_model.fit(\\n\",\n",
    "    \"    X_train_scaled, y_train,\\n\",\n",
    "    \"    epochs=50,\\n\",\n",
    "    \"    batch_size=32,\\n\",\n",
    "    \"    validation_split=0.2,\\n\",\n",
    "    \"    verbose=1,\\n\",\n",
    "    \"    callbacks=[\\n\",\n",
    "    \"        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\\n\",\n",
    "    \"        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\\n\",\n",
    "    \"    ]\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate neural network\\n\",\n",
    "    \"nn_predictions = nn_model.predict(X_test_scaled)\\n\",\n",
    "    \"nn_pred_binary = (nn_predictions > 0.5).astype(int).flatten()\\n\",\n",
    "    \"nn_pred_proba = nn_predictions.flatten()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate metrics\\n\",\n",
    "    \"nn_accuracy = accuracy_score(y_test, nn_pred_binary)\\n\",\n",
    "    \"nn_precision = precision_score(y_test, nn_pred_binary)\\n\",\n",
    "    \"nn_recall = recall_score(y_test, nn_pred_binary)\\n\",\n",
    "    \"nn_f1 = f1_score(y_test, nn_pred_binary)\\n\",\n",
    "    \"nn_auc = roc_auc_score(y_test, nn_pred_proba)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüß† NEURAL NETWORK PERFORMANCE:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 40)\\n\",\n",
    "    \"print(f\\\"‚úÖ Accuracy: {nn_accuracy:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"üìä Precision: {nn_precision:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"üéØ Recall: {nn_recall:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"‚öñÔ∏è F1-Score: {nn_f1:.3f}\\\")\\n\",\n",
    "    \"print(f\\\"üìà AUC: {nn_auc:.3f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot training history\\n\",\n",
    "    \"fig, axes = plt.subplots(1, 2, figsize=(15, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Loss\\n\",\n",
    "    \"axes[0].plot(history.history['loss'], label='Training Loss')\\n\",\n",
    "    \"axes[0].plot(history.history['val_loss'], label='Validation Loss')\\n\",\n",
    "    \"axes[0].set_title('üìâ Model Loss')\\n\",\n",
    "    \"axes[0].set_xlabel('Epoch')\\n\",\n",
    "    \"axes[0].set_ylabel('Loss')\\n\",\n",
    "    \"axes[0].legend()\\n\",\n",
    "    \"axes[0].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Accuracy\\n\",\n",
    "    \"axes[1].plot(history.history['accuracy'], label='Training Accuracy')\\n\",\n",
    "    \"axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\\n\",\n",
    "    \"axes[1].set_title('üìà Model Accuracy')\\n\",\n",
    "    \"axes[1].set_xlabel('Epoch')\\n\",\n",
    "    \"axes[1].set_ylabel('Accuracy')\\n\",\n",
    "    \"axes[1].legend()\\n\",\n",
    "    \"axes[1].grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üéØ Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature importance from best traditional model\\n\",\n",
    "    \"best_traditional_model = model_results[best_model_name]['model']\\n\",\n",
    "    \"\\n\",\n",
    "    \"if hasattr(best_traditional_model, 'feature_importances_'):\\n\",\n",
    "    \"    feature_importance = best_traditional_model.feature_importances_\\n\",\n",
    "    \"    feature_names = X.columns\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create feature importance dataframe\\n\",\n",
    "    \"    importance_df = pd.DataFrame({\\n\",\n",
    "    \"        'feature': feature_names,\\n\",\n",
    "    \"        'importance': feature_importance\\n\",\n",
    "    \"    }).sort_values('importance', ascending=False)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot feature importance\\n\",\n",
    "    \"    plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"    bars = plt.barh(importance_df['feature'], importance_df['importance'])\\n\",\n",
    "    \"    plt.xlabel('Feature Importance')\\n\",\n",
    "    \"    plt.title(f'üéØ Feature Importance - {best_model_name}')\\n\",\n",
    "    \"    plt.gca().invert_yaxis()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Color bars based on importance\\n\",\n",
    "    \"    colors = plt.cm.RdYlGn(importance_df['importance'] / importance_df['importance'].max())\\n\",\n",
    "    \"    for bar, color in zip(bars, colors):\\n\",\n",
    "    \"        bar.set_color(color)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(\\\"üéØ FEATURE IMPORTANCE RANKING:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 35)\\n\",\n",
    "    \"    for i, (_, row) in enumerate(importance_df.iterrows(), 1):\\n\",\n",
    "    \"        print(f\\\"{i:2d}. {row['feature'].replace('_', ' ').title():<20} {row['importance']:.3f}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Feature importance not available for {best_model_name}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîç Model Interpretation and Examples\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create example predictions with explanations\\n\",\n",
    "    \"def explain_prediction(model, scaler, sample_features, feature_names, use_scaling=True):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Explain a single prediction with feature contributions\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    if use_scaling:\\n\",\n",
    "    \"        sample_scaled = scaler.transform([sample_features])\\n\",\n",
    "    \"        prediction = model.predict_proba(sample_scaled)[0][1]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        prediction = model.predict_proba([sample_features])[0][1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return prediction\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test examples\\n\",\n",
    "    \"test_examples = [\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'name': 'Suspicious Modern Painting',\\n\",\n",
    "    \"        'features': [0.35, 4.2, 0.45, 0.1, 0.3, 0.2, 0.3, 0.1],\\n\",\n",
    "    \"        'description': 'High color variance, low texture complexity, poor provenance'\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'name': 'Authentic Renaissance Work',\\n\",\n",
    "    \"        'features': [0.12, 7.8, 0.85, 0.8, 0.9, 0.95, 0.9, 0.8],\\n\",\n",
    "    \"        'description': 'Natural aging, consistent brush work, excellent provenance'\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    {\\n\",\n",
    "    \"        'name': 'Questionable Attribution',\\n\",\n",
    "    \"        'features': [0.2, 6.5, 0.7, 0.6, 0.7, 0.8, 0.5, 0.4],\\n\",\n",
    "    \"        'description': 'Mixed signals - some authentic features, poor style match'\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üîç EXAMPLE PREDICTIONS AND EXPLANATIONS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for example in test_examples:\\n\",\n",
    "    \"    # Get predictions from best models\\n\",\n",
    "    \"    traditional_pred = explain_prediction(\\n\",\n",
    "    \"        best_traditional_model, scaler, example['features'], \\n\",\n",
    "    \"        X.columns, use_scaling=(best_model_name in ['SVM', 'Logistic Regression'])\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if best_model_name in ['SVM', 'Logistic Regression']:\\n\",\n",
    "    \"        sample_scaled = scaler.transform([example['features']])\\n\",\n",
    "    \"        nn_pred = nn_model.predict(sample_scaled)[0][0]\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        sample_scaled = scaler.transform([example['features']])\\n\",\n",
    "    \"        nn_pred = nn_model.predict(sample_scaled)[0][0]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nüé® {example['name']}\\\")\\n\",\n",
    "    \"    print(f\\\"üìã Description: {example['description']}\\\")\\n\",\n",
    "    \"    print(f\\\"ü§ñ {best_model_name} Authenticity Score: {traditional_pred:.3f} ({'‚úÖ AUTHENTIC' if traditional_pred > 0.5 else '‚ùå SUSPICIOUS'})\\\")\\n\",\n",
    "    \"    print(f\\\"üß† Neural Network Score: {nn_pred:.3f} ({'‚úÖ AUTHENTIC' if nn_pred > 0.5 else '‚ùå SUSPICIOUS'})\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Feature breakdown\\n\",\n",
    "    \"    print(\\\"üìä Feature Analysis:\\\")\\n\",\n",
    "    \"    for i, (feature, value) in enumerate(zip(X.columns, example['features'])):\\n\",\n",
    "    \"        status = \\\"‚úÖ\\\" if (\\n\",\n",
    "    \"            (feature in ['texture_complexity', 'brush_consistency', 'age_indicators', \\n\",\n",
    "    \"                        'canvas_authenticity', 'pigment_match', 'style_match', 'provenance_score'] and value > 0.6) or\\n\",\n",
    "    \"            (feature == 'color_variance' and value < 0.2)\\n\",\n",
    "    \"        ) else \\\"‚ö†Ô∏è\\\"\\n\",\n",
    "    \"        print(f\\\"  {status} {feature.replace('_', ' ').title()}: {value:.2f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üíæ Model Deployment Preparation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save the best models\\n\",\n",
    "    \"import joblib\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create models directory\\n\",\n",
    "    \"os.makedirs('../models', exist_ok=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save traditional model\\n\",\n",
    "    \"joblib.dump(best_traditional_model, f'../models/best_traditional_model_{best_model_name.lower().replace(\\\" \\\", \\\"_\\\")}.pkl')\\n\",\n",
    "    \"joblib.dump(scaler, '../models/feature_scaler.pkl')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save neural network\\n\",\n",
    "    \"nn_model.save('../models/authenticity_neural_network.h5')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save feature names\\n\",\n",
    "    \"feature_info = {\\n\",\n",
    "    \"    'feature_names': list(X.columns),\\n\",\n",
    "    \"    'feature_descriptions': {\\n\",\n",
    "    \"        'color_variance': 'Variation in color distribution (lower = more natural)',\\n\",\n",
    "    \"        'texture_complexity': 'Complexity of surface texture patterns',\\n\",\n",
    "    \"        'brush_consistency': 'Consistency of brush stroke patterns',\\n\",\n",
    "    \"        'age_indicators': 'Presence of natural aging signs',\\n\",\n",
    "    \"        'canvas_authenticity': 'Authenticity of canvas/material',\\n\",\n",
    "    \"        'pigment_match': 'Match with period-appropriate pigments',\\n\",\n",
    "    \"        'style_match': 'Consistency with artist\\\\'s known style',\\n\",\n",
    "    \"        'provenance_score': 'Quality of historical documentation'\\n\",\n",
    "    \"    },\\n\",\n",
    "    \"    'model_performance': {\\n\",\n",
    "    \"        'best_traditional_model': best_model_name,\\n\",\n",
    "    \"        'traditional_auc': comparison_df.iloc[0]['AUC'],\\n\",\n",
    "    \"        'neural_network_auc': nn_auc,\\n\",\n",
    "    \"        'recommendation': 'Use ensemble of both models for critical decisions'\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"with open('../models/model_info.json', 'w') as f:\\n\",\n",
    "    \"    json.dump(feature_info, f, indent=2)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üíæ MODELS SAVED SUCCESSFULLY!\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 35)\\n\",\n",
    "    \"print(f\\\"üìÅ Best Traditional Model: {best_model_name}\\\")\\n\",\n",
    "    \"print(f\\\"üß† Neural Network: Saved as authenticity_neural_network.h5\\\")\\n\",\n",
    "    \"print(f\\\"üìä Feature Scaler: Saved for preprocessing\\\")\\n\",\n",
    "    \"print(f\\\"üìã Model Info: Complete metadata saved\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüöÄ DEPLOYMENT READY!\\\")\\n\",\n",
    "    \"print(\\\"Models can be loaded and used for:\\\")\\n\",\n",
    "    \"print(\\\"- Real-time artwork authentication\\\")\\n\",\n",
    "    \"print(\\\"- Batch processing of museum collections\\\")\\n\",\n",
    "    \"print(\\\"- Integration with digital preservation systems\\\")\\n\",\n",
    "    \"print(\\\"- API services for art dealers and collectors\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìù Model Performance Summary\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Final summary\\n\",\n",
    "    \"print(\\\"üé® DIGITAL ALEXANDRIA - AUTHENTICITY DETECTION MODEL\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 60)\\n\",\n",
    "    \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
